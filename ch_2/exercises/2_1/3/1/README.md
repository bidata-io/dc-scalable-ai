# Configuring the Optimizer

## Instructions

- Return an Adam optimizer with a learning rate of 1e-3 for training your model.

<details>
  
<summary><h2>Hint</h2></summary>

Define the `configure_optimizers` method to return an Adam optimizer with a specified learning rate.
  
</details>

## Solution

[solution](https://github.com/bidata-io/dc-scalable-ai/blob/main/ch_2/exercises/2_1/3/1/solution.py)

## Sample Code

```python
# N/A
```

## Submission Correctness Tests (SCT)

[sct](https://github.com/bidata-io/dc-scalable-ai/blob/main/ch_2/exercises/2_1/3/1/sct.py)
